# Project 7: Word Relationship and Geographic Data Clustering, Fall 2025

See the [details document](docs/details.md) for information on using Git, starting the project, and more details about the project including information about the classes and concepts that are outlined briefly below. You'll absolutely need to read the information in the details document to understand how the classes in this project work independently and together. The details document also contains project-specific coding help. This current document provides a high-level overview of the assignment.

You are STRONGLY encouraged to work with a partner on P7! (See the [details document](docs/details.md) for information on using Git with a partner and how the workflow can proceed.)

## Project Introduction

This project is designed to introduce some concepts from AI and  _NLP_ (Natural Language Processing) in the context of two programs you'll write. In contrast to most of the projects you've completed in 201, for these two programs you'll need to write all the code with minimal guidance as to what instance variables and helper methods are appropriate. You'll be given some method headers to facilitate the Gradescope autograders.

If you'd like a reasonably complete guide to the AI/NLP/ML concepts here, see
[Chapter 5, Embeddings, Jurafsky NLP book](https://courses.cs.duke.edu/compsci201/fall25/assign/jurafsky-chap5-2025.pdf). You can find that complete work using "Jurafsky NLP book" in your favorite search engine.

## Outline 

- Implement the class `FindSimilarities` to read a file of word embeddings generated by training an AI/NLP model and find words that are semantically close to a given word. See explanation below for more details.

- Implement the class `DBScanClusters` to process geographic data from *P0: Person201* and determine what _clusters_ exist using the DBSCAN (Density-based spatial clustering of applications with noise) algorithm with different parameters.


## Part 1: Implementing `FindSimilarities`

Complete two methods: one to read a data file of word embeddings and one to find those words that are semantically close to a given word. You must use the method headers given below since they are used both in the JUnit tests provided in the assignment you fork and clone, and also in the Gradescope Autograder.

### Reading a file

Data files will be in the same .csv (comma separated values) format with each line containing a word/String followed by some number of double values (this is a text file, you'll likely read a line as a String and call `.split(",")` on the line).  Every line will have the first string be a word, which will be followed by a fixed number, e.g., 300, of double values that represent a semantic embedding of that word.

```
happy,-5.18798828e-04,2.5828e-04,...,1.339e-04
sad, -3.1839393e-0.4, 2.833838e-04, ...,-2.9393e-04
...
```
Note the method header below has return type `void`. This method reads the file and stores information
appropriately in instance variables to enable the next method to work as specified.
```
    public void read(String fileName) throws IOException
```

#### Reading Code

You will find code to read a file using a Scanner in the `Person201Utilities` class that's been copied into this project from the P0 project, as well as in *P3:DNABenchmark.dnaFromScanner*. We ask that you use these as a model for writing your reading code rather than asking an LLM to write it. 
<!--If you do ask an LLM, you're expected to document the code contributions of the LLM in a form you're asked
to submit for the analysis section.-->
If you do ask an LLM, you must document the code contributions of the LLM in [this form](https://forms.office.com/r/3aFh2THspn).


### Finding Semantically Similar words

The method below will use instance variables to find the embedding vector for `key` (if that
exists) and the finds the closest `n` words to `key` using cosine similarity (use the library
method in `VectorUtils.cosineSimilarity`).

You can assume that if `key` was found in the reading then there will be at least `n` words, i.e., the code will not be tested with values of `n` that are greater than the number of words read. It is possible that `key` was not in the read/training data, in which case a zero-sized array should be returned.
```
    public List<WordDouble> getSimilar(String key, int n)
```
You'll likely want/need to look at the `WordDouble` class in the project as you write `getSimilar`. If you decide to sort `WordDouble` objects
based on the `double` value 
of the field `measure` you will find it **extremely helpful** to use the `Double.compare` comparator that compares `double` values and returns an `int` value 
that all comparators must return based on comparing `double` values.

### JUnit tests

You can test your code using the JUnit tests in `TestCosine.java`, invoked from the beaker/testing icon. These are the tests that will be in the Gradescope autograder.

## Part 2: Implementing `DBScanClusters`

For this part of P7, you'll implement a method (and likely several helper methods, as you choose) to
implement the [DBSCAN algorithm](https://en.wikipedia.org/wiki/DBSCAN) for finding clusters
based on geographic data obtained from the P0:Person201 assignment. 

You'll write one method that returns a list of clusters based on the geographic data
in parameter `people` and using `epsilon` and `minPoints` in accordance with the DBSCAN
algorithm. We discussed this algorithm in class and you'll find more information
in the [details document](docs/details.md).

```
public List<Set<Person201>> getClusters(Person201[] people, 
                                        double epsilon, int minPoints)
```

You will almost certainly want to write helper methods based on the steps of the DBSCAN
algorithm. That's your design decision, but again, see the steps of the algorithm in the
[details document](docs/details.md). 

### JUnit tests

The code in `TestDBScan` can be used to test your code. It will use the same tests
that are in the Gradescope/Autograder assignment.

### Creating a map

You can visualize a cluster in a dynamic map by running the code in `CreateClusterMap.java`. That 
file uses your DBSCAN `getClusters` code and creates an `ola.json` file that is read by the `map.html` file to create the JavaScript/HTML visualization.
You may decide, appropriately, to rename the data file to your own `netid.json`, in which case you'll need to find `ola.json` in the `map.html` file and rename that. 

To test your visualization you'll need a web-server. The simplest way to do this requires having Python installed. Navigate to your P7 folder. Typing `ls` should show you
`ola.json`, `map.html`, and folders `src`, `lib`, `data`, and `docs`.  Then type:
```
   python -m http.server 8000
```
or use python3 if you have it. Then open a browser and type `localhost:8000` as the URL. This should show
you the `map.html` file. Open this and you should see your visualization. If you rerun `CreateClusterMap` with different `epsilon` and `minPoints` parameters, simply reload the `map.html` file
in your browser.


## Analysis

You will *NOT* submit answers to these questions, but the questions and information could be something you'll see on the final. You **will need to fill out 
a form** describing what you consulted in completing this assignment and how (if at all) you used LLMs. [The form is linked here](https://forms.office.com/r/3aFh2THspn).


**Question 1.** 

For `DBScanClusters` given an epsilon, say 100km, do you expect to find more clusters or fewer clusters
by increasing the value of `minPoints`? Explain. Ideally you can test your answer by running code.

**Question 2.** 

For `FindSimilarities`, if a word `W` appears in the closest 5 words to word `V`, will it always
be the case that `V` appears in the closet 5 words of `W`? Explain. To be more specific,
if `V` is the semantically closest word to `W`, does that mean `W` is the closest to `V`? Ideally you could run an experiment with your code to verify your reasoning.

**Question 3.**  

Explain in as few words as you can, but thoroughly, the difference between a core point and
a border/fringe point in the DBSCAN algorithm and code.

## Submitting and Grading

Push your code to Git. Do this often. To submit:

Submit your code on gradescope to the autograder. If you are working with a partner, refer to [this document](https://docs.google.com/document/d/e/2PACX-1vREK5ajnfEAk3FKjkoKR1wFtVAAEN3hGYwNipZbcbBCnWodkY2UI1lp856fz0ZFbxQ3yLPkotZ0U1U1/pub) for submitting to Gradescope with a partner. 

Points:

1. `FindSimilarities` is 12 points
2. `DBScanClusters` is 12 points
3. Fillout the LLM/Example form is 4 points.


